= Operational Intelligence Multi-Product Lab

. https://etherpad.net/p/RHTE-Operational-Intelligence                      :   etherpad   
. https://github.com/redhat-gpe/rhte_lab_06_operational_intelligence        :   lab instructions
. https://github.com/gpe-mw-training/operational_intelligence               :   lab assets

== Related session(s) at RHT Tech Exchange

. link:https://www.youtube.com/watch?v=-izxHJQSQ7E[Data streaming with Apache Kafka] 
+
Speakers at each Tech Exchange are TBD

:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:

= Operational Intelligence and Apache Spark Lab 

.Goals
* Apache Spark Deployment on Oshinko Cluster
* Uber Data Analysis and Surge Prediction 
* End-to-end Monitoring of Real Time Uber Data using Apache Spark,Kafka,Spark MLLIB.
* Visualization of Real Time Monitoring Data using Interactive Notebooks Zeppelin and Juypter Notebook.

.Prerequisite
* Skills
** Programming Knowledge of Python, Scala and Java Languages.
** Basic Knowledge of Apache Spark.
** Knowledge on OpenShift Deployments
* Tools
** `curl` utility
** `sed` utility
** `PIP Install` utility
** `Interactive Notebooks - Jupyter and Zeppelin`
** `oc` version 3.9 utility

= How is this Lab Structured?
* Basic of Data Analytics.
* Apache Spark Deployment on Oshinko cluster (Simple Exercise)
* Usage of Interactive Notebooks (Zeppelin and Jupyter) on Uber Near Time Data Analysis (Medium Exercise).
* Structured Streaming of Uber Data using Apache Spark Streaming and Kafka API (Complex Exercise).

:numbered:


== Overview (Simple Exercise - Lab 1)

This Project shows how Spark is integrated with RedHatDecision Manager (Drools), Business use case of Simple Credit Approval Logic,  When the credit score is greater than 600, Approve the loan. 

The purpose of this lab is to apply both a Drools and Apache Spark deployment on Oshinko Cluster.

== Why do we need this Simple Exercise Lab and how the students get benefitted by this lab?
This Lab helps the students to get to know the basics of cluster level deployment of Spark Master/Worker node configuration.

Basic deployment of spark jobs on Oshinko cluster.

Integration between Drools and Apache Spark, so that on how the rules getting fired while spark is executing the job.

Maven Deployment.

=== Background

Apache Spark Performs sequences of Operations like Inputting the data -> Filter the data -> Tag the Data (Business Logics like loan defaulters ) -> Model -> as the useful business data. Hence huge amount of Java Code (rule logic)in tagging the data for writing/cleansing the data for different transactions and scenarios which is not necessary.

Instead you can use the RedHatDecisionManager which completes the seperates the business logic with the application scope and make the clear abstractions with business rules and application logic. Developer can easily maintain these rules in RedHatDecision Manager.

Apart from these Spark Provides several performance benefits by handling and parallelly processing huge amount of data and also in near-realtime transactions.

=== Reference

. Youtube video https://www.redhat.com/en/about/videos/red-hat-consulting-decision-manager-and-apache-spark-integration. 

=== Alternatives

In regards to Apache Spark , the following are related community initiatives that are not covered in this lab.

==== Scala Programming Language
 
Scala is an Excellent functional programming JVM Language which is currently used in wide spread.
However we will cover the basic topics in Scala.
 
==== Python Programming Language

Python is an Excellent Language which currently dominates the data analytics world.
However we will cover the basic topics in Python and Machine Learning Libraries in Python.


==== Zeppelin Notebook

Since we use Zeppelin Notebook for data visualization. Advanced Interpretor and their installation in Zeppelin notebook is not covered.

Outcomes of this working group may potentially guide the development and roadmap of future releases of the RadAnalytics product.
 
== Lab Asset Overview

=== Environment Variables

Before getting started, you'll want to open a terminal window and set the following environment variables that will be used throughout the duration of this lab.

ifdef::showscript[]
If student lab environment and Oshinko Cluster were provisioned using the ocp-workload-rhte-mw-api-mesh ansible role, then student details can be found in:

/tmp/user_info_file.txt

endif::showscript[]

-----
######  Instructor will provide the values to these environment variables #######

$ export REGION=<provided by your instructor>
$ export GUID=<provided by your instructor>
$ export OCP_PASSWD=<provided by your instructor>
$ export API_ADMIN_ACCESS_TOKEN=<provided by your instructor>



#######  Using above variables, the following can be copied and pasted in same terminal window   ########

$ export OCP_WILDCARD_DOMAIN=apps.$REGION.openshift.opentlc.com
$ export TENANT_NAME=$OCP_USERNAME-rules

$ export OCP_USERNAME=user$GUID
$ export OCP_PROJECT=rhte-mw-api-rules-$GUID
-----

ifdef::showscript[]

# Potential alternative using service endpoint (may need to use master)

endif::showscript[]

=== Oshinko Cluster Access

Your lab environment includes access to a Oshinko Cluster Environment.

For the purpose of this lab, you will serve as the administrator of your own rules _tenant_ (aka: _domain_)

. Log into the Openshift portal of your dedicated clustered environment using the information to do provided by your instructor

. Instructor will assig you the user code to access the dedicated clustered environment, point to your browser to the output of the following:
+
-----
$ echo -en "\n\nhttps://$TENANT_NAME-rules.$OCP_WILDCARD_DOMAIN\n\n"
-----

. Authenticate using the values of $OCP_USERNAME and $OCP_PASSWD   (Use your OCP credentials).
+
image::images/3scale_login.png[]


=== OpenShift access

You lab environment is built on Red Hat's OpenShift Container Platform.

Access to your OCP resources can be gained via both the `oc` utility as well as the OCP web console.

. Log into OpenShift

-----
$ oc login https://master.$REGION.openshift.opentlc.com -u $OCP_USERNAME -p $OCP_PASSWD
-----

. Ensure that your `oc` client is the same minor release version as the server:
+
-----
$ oc version

oc v3.9.30
kubernetes v1.9.1+a0ce1bc657
features: Basic-Auth GSSAPI Kerberos SPNEGO

Server https://master.a4ec.openshift.opentlc.com:443
openshift v3.9.31
kubernetes v1.9.1+a0ce1bc657
-----

.. In the above example, notice that version of the `oc` client is of the same minor release (v3.9.30) of the OpenShift server (v3.9.31)
.. There a known subtle problems with using a version of the `oc` client that is different from your target OpenShift server.

. View existing projects:
+
-----
$ oc get projects

... 
rhte-mw-rules-user1        rhte-mw-rules-user1        Active
-----

.. Your OCP user has been provided with _view_ and _edit_ access to the oshinko-cluster-namespace with all _control plane_ functionality.
+
Later in this lab, you'll use a utility called oshiko-cluster .
This utility will need both view and edit privileges to the namespace.

.. Your OCP use has also been provided with _view_ access to a multi-tenant 

.. The namespace _rhte-mw-rules-user1-*_ is where you will be working throughout the duration of this lab.

. Switch to your  OpenShift project
+
-----
$ oc project $OCP_PROJECT
$ oc project rhte-mw-rules-user1
-----

. Log into OpenShift Web Console
.. Many OpenShift related tasks found in this lab can be completed in the Web Console (as an alternative to using the `oc` utility`.
.. To access, point to your browser to the output of the following:
+
-----
$ echo -en "\n\nhttps://master.$REGION.openshift.opentlc.com\n\n"
-----

.. Authenticate using the values of $OCP_USERNAME and $OCP_PASSWD
.. $OCP_USERNAME will be provided by your instructor (user5 - user 95) and $OCP_PASSWD is r3dh4t1!


[[dvsdc]]
=== Deployment vs DeploymentConfig 

Your lab assets consist of a mix of OpenShift _Deployment_ and _DeploymentConfig_ resources.

The _Deployment_ construct is a more recent Kubernetes equivalent of what has always been in OpenShift:  _DeploymentConfig_.


=== Spark Drools Project

This Project will show how to use the spark job to fire the rules.
Simple Credit Approval logic, if the Applicant Credit score is greater than 600, fire the rules (approve the loan). 
Approval.drl (drool file) holds the business logic.
Project can be easily imported as an Maven Project and fitted to be running in any ide.



==== Deployment in Openshift - Oshinko Cluster

-----
$ oc new-project  -n $OCP_PROJECT

...

-----

. Create Deployment Objects using Template
+
-----
$ oc create -f https://raw.githubusercontent.com/Pkrish15/spark-drools/master/resources.yaml 

...

serviceaccount "oshinko" created
rolebinding "oshinko-edit" created
template "oshinko-python-spark-build-dc" created
template "oshinko-java-spark-build-dc" created
template "oshinko-scala-spark-build-dc" created
template "oshinko-webui-secure" created
template "oshinko-webui" created

-----

. Apply the webui template, this will pull the oshinko cluster to every student's desktop
+
-----
$ oc new-app oshinko-webui 

...
--> Deploying template "rhte-mw-rules-user1/oshinko-webui" to project rhte-mw-rules-user1

     * With parameters:
        * SPARK_DEFAULT=
        * OSHINKO_WEB_NAME=oshinko-web
        * OSHINKO_WEB_IMAGE=radanalyticsio/oshinko-webui:stable
        * OSHINKO_WEB_ROUTE_HOSTNAME=
        * OSHINKO_REFRESH_INTERVAL=5

--> Creating resources ...
    service "oshinko-web-proxy" created
    service "oshinko-web" created
    route "oshinko-web" created
    deploymentconfig "oshinko-web" created
--> Success
    Access your application via route 'oshinko-web-rhte-mw-rules-user1.apps.6d13.openshift.opentlc.com' 
    Run 'oc status' to view your app.
...
-----

. Get the Routes and Access the URL.
-----
...
NAME                HOST/PORT                                                         PATH      SERVICES                            PORT            TERMINATION   WILDCARD
oshinko-web         oshinko-web-rhte-mw-rules-user1.apps.6d13.openshift.opentlc.com   /webui    oshinko-web(50%),oshinko-web(50%)   <all>                         None
oshinko-web-proxy   oshinko-web-rhte-mw-rules-user1.apps.6d13.openshift.opentlc.com   /proxy    oshinko-web-proxy                   oc-proxy-port                 None
...
-----

. Using the S2I Image to build and deploy the Project.
+
----
oc new-app --template oshinko-java-spark-build-dc \
    -p APPLICATION_NAME=spark-drools \
    -p APP_MAIN_CLASS=com.redhat.gpte.App \
    -p GIT_URI=https://github.com/Pkrish15/spark-drools \
    -p APP_FILE=spark-drools.jar \
    -p OSHINKO_DEL_CLUSTER=false

----
----
...
--> Deploying template "rhte-mw-rules-user1/oshinko-java-spark-build-dc" to project rhte-mw-rules-user1

     Apache Spark Java
     ---------
     Create a buildconfig, imagestream and deploymentconfig using source-to-image and Java Spark source files hosted in git'

     * With parameters:
        * APPLICATION_NAME=spark-drools
        * GIT_URI=https://github.com/Pkrish15/spark-drools
        * GIT_REF=master
        * CONTEXT_DIR=
        * APP_FILE=spark-drools
        * APP_ARGS=
        * APP_MAIN_CLASS=com.redhat.gpte.App
        * SPARK_OPTIONS=
        * OSHINKO_CLUSTER_NAME=
        * OSHINKO_NAMED_CONFIG=
        * OSHINKO_SPARK_DRIVER_CONFIG=
        * OSHINKO_DEL_CLUSTER=false

--> Creating resources ...
    imagestream "spark-drools" created
    buildconfig "spark-drools" created
    deploymentconfig "spark-drools" created
    service "spark-drools" created
    service "spark-drools-headless" created
--> Success
    Build scheduled, use 'oc logs -f bc/spark-drools' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/spark-drools' 
     'oc expose svc/spark-drools-headless' 
    Run 'oc status' to view your app.

...
----

.Sample Output
-----
...

18/08/19 18:13:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1881 ms on localhost (executor driver) (2/16)
18/08/19 18:13:34 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 1792 ms on localhost (executor driver) (3/16)
18/08/19 18:13:34 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1798 ms on localhost (executor driver) (4/16)
18/08/19 18:13:34 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 1796 ms on localhost (executor driver) (5/16)
18/08/19 18:13:34 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 1794 ms on localhost (executor driver) (6/16)
18/08/19 18:13:34 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 746 bytes result sent to driver
18/08/19 18:13:34 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 789 bytes result sent to driver
18/08/19 18:13:34 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 746 bytes result sent to driver
18/08/19 18:13:34 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 2092 ms on localhost (executor driver) (7/16)
18/08/19 18:13:34 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 746 bytes result sent to driver
18/08/19 18:13:34 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 746 bytes result sent to driver
18/08/19 18:13:34 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 2090 ms on localhost (executor driver) (8/16)
18/08/19 18:13:34 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 2096 ms on localhost (executor driver) (9/16)
18/08/19 18:13:34 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 2092 ms on localhost (executor driver) (10/16)
18/08/19 18:13:34 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2099 ms on localhost (executor driver) (11/16)
18/08/19 18:13:34 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 746 bytes result sent to driver
18/08/19 18:13:34 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 746 bytes result sent to driver
18/08/19 18:13:34 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 746 bytes result sent to driver
18/08/19 18:13:34 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 746 bytes result sent to driver
18/08/19 18:13:34 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 746 bytes result sent to driver
18/08/19 18:13:34 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 2293 ms on localhost (executor driver) (12/16)
18/08/19 18:13:34 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 2291 ms on localhost (executor driver) (13/16)
18/08/19 18:13:34 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2296 ms on localhost (executor driver) (14/16)
18/08/19 18:13:34 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 2296 ms on localhost (executor driver) (15/16)
18/08/19 18:13:34 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 2294 ms on localhost (executor driver) (16/16)
18/08/19 18:13:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/08/19 18:13:34 INFO DAGScheduler: ResultStage 0 (count at App.java:53) finished in 2.900 s
18/08/19 18:13:34 INFO DAGScheduler: Job 0 finished: count at App.java:53, took 3.187579 s
Number of applicants approved: 5
18/08/19 18:13:35 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
18/08/19 18:13:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/08/19 18:13:35 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
18/08/19 18:13:35 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:78) with 16 output partitions
18/08/19 18:13:35 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:78)
18/08/19 18:13:35 INFO DAGScheduler: Parents of final stage: List()
18/08/19 18:13:35 INFO DAGScheduler: Missing parents: List()
18/08/19 18:13:35 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at saveAsTextFile at App.java:56), which has no missing parents
18/08/19 18:13:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 70.8 KB, free 366.1 MB)
18/08/19 18:13:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.1 KB, free 366.1 MB)
18/08/19 18:13:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-drools-headless:44636 (size: 25.1 KB, free: 366.3 MB)
18/08/19 18:13:35 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039
18/08/19 18:13:35 INFO DAGScheduler: Submitting 16 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at saveAsTextFile at App.java:56) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
18/08/19 18:13:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 16 tasks
18/08/19 18:13:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 7855 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 17, localhost, executor driver, partition 1, PROCESS_LOCAL, 8008 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 18, localhost, executor driver, partition 2, PROCESS_LOCAL, 7855 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 19, localhost, executor driver, partition 3, PROCESS_LOCAL, 8010 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 20, localhost, executor driver, partition 4, PROCESS_LOCAL, 8006 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 21, localhost, executor driver, partition 5, PROCESS_LOCAL, 7855 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 22, localhost, executor driver, partition 6, PROCESS_LOCAL, 8006 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 23, localhost, executor driver, partition 7, PROCESS_LOCAL, 8004 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 24, localhost, executor driver, partition 8, PROCESS_LOCAL, 7855 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 25, localhost, executor driver, partition 9, PROCESS_LOCAL, 8005 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 26, localhost, executor driver, partition 10, PROCESS_LOCAL, 7855 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 27, localhost, executor driver, partition 11, PROCESS_LOCAL, 8004 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 28, localhost, executor driver, partition 12, PROCESS_LOCAL, 8008 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 29, localhost, executor driver, partition 13, PROCESS_LOCAL, 7855 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 30, localhost, executor driver, partition 14, PROCESS_LOCAL, 8007 bytes)
18/08/19 18:13:35 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 31, localhost, executor driver, partition 15, PROCESS_LOCAL, 8009 bytes)
18/08/19 18:13:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 16)
18/08/19 18:13:35 INFO Executor: Running task 4.0 in stage 1.0 (TID 20)

...
-----

==== Implementation Flow Diagram

WIP

=== OpenShift Console URL (Oshinko Cluster Environment)

WIP
------------------------------------------------End of Lab 1----------------------------------------------------------------
# Data Discovery Phase  - First Phase (Medium Level Project - Lab 2)
# Analytics using the Data Model - Second Phase 
It use Spark Streaming and Rules Implementation.WIP <br>
# Where do we get these DataSets? <br>
http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml <br>
http://www.nyc.gov/html/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf <br>
http://data.beta.nyc/dataset/uber-trip-data-foiled-apr-sep-2014 <br>
https://developer.uber.com/docs/businesses/data-automation/data-download <br>


#  Detailed Explaination of the first phase - Data Discovery Phase

1) Why this phase is needed? <br>
   This phase is needed to study about the historical data, and observe the pattern recognition of the uber system which is needed. Based on this we can arrive a conclusion for better decision making and predictions.<br>
 
 2) What is analytics using the model? <br>
   This is the second phase of the project,uses the model in production on live events, it still needed to do an analyis of historical data. <br>
![alt text](https://github.com/Pkrish15/uber-datanalysis/blob/master/1.jpg)<br>

3) What Algorithm choosed suitable for Data Analytics? <br>

Clustering uses unsupervised algorithms, which do not have the outputs (labeled data) in advance. K-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters (k). Clustering using the K-means algorithm begins by initializing all the coordinates to k number of centroids. With every pass of the algorithm, each point is assigned to its nearest centroid based on some distance metric, which is usually Euclidean distance. The centroids are then updated to be the “centers” of all the points assigned to it in that pass. This repeats until there is a minimum change in the centers. <br>

4) What is in the data set? <br>

The Data Set Schema
Date/Time: The date and time of the Uber pickup
Lat: The latitude of the Uber pickup
Lon: The longitude of the Uber pickup
Base: The TLC base company affiliated with the Uber pickup
​​The Data Records are in CSV format. An example line is shown below:

2014-08-01 00:00:00,40.729,-73.9422,B02598 <br>

5) How do we do it? <br>

Load the data into a Spark Data Frame <br>

![alt text](https://github.com/Pkrish15/uber-datanalysis/blob/master/2.png)<br> <br>

Define Features Array <br>
In order for the features to be used by a machine learning algorithm, the features are transformed and put into Feature Vectors, which are vectors of numbers representing the value for each feature. Below, a VectorAssembler is used to transform and return a new DataFrame with all of the feature columns in a vector column. <br>

![alt text](https://github.com/Pkrish15/uber-datanalysis/blob/master/3.png)<br> <br>

Create a KMeans Object, set the parameters to define the number of clusters and the maximum number of iterations to determine the clusters and then we fit our model to the input data.

![alt text](https://github.com/Pkrish15/uber-datanalysis/blob/master/4.png)<br> <br>

Output, Cluster Centers are displayed on the Google Map <br>
![alt text](https://github.com/Pkrish15/uber-datanalysis/blob/master/5.png)<br> <br>

Further Analysis of cluster <br>
![alt text](https://github.com/Pkrish15/uber-datanalysis/blob/master/6.png)<br> <br>

6) What are the Analysis Questions? <br>

Which hour of the day and which cluster had highest number of pickups?<br>

How many pickups occured in each cluster? <br>

7) How can we deploy in OpenShift? <br>

# Implementation on Openshift <br>
   a) oc new-project pk-zp <br> <br>
   b) oc create -f https://raw.githubusercontent.com/Pkrish15/uber-datanalysis/master/zeppelin-openshift.yaml <br> <br>
   c) oc new-app --template=$namespace/apache-zeppelin-openshift \
    --param=APPLICATION_NAME=apache-zeppelin \
    --param=GIT_URI=https://github.com/rimolive/zeppelin-notebooks.git \
    --param=ZEPPELIN_INTERPRETERS=md       <br><br>
 
 2) Create a PVC in the pod claiming 2GiB <br><br>
 3) Copy the Local Data to the Pod Directory using Rsync Command <br><br>
     oc rsync /home/prakrish/workspace/uberdata-analysis/src/main/resources/data/  apache-zeppelin-2-f89tz:/data <br>
     oc rsync src directory pod directory:/data This is a format <br> <br>
 
 4) Open the Zeppellin notebook <br> <br>
    oc get route <br>
    http://apache-zeppelin-pk-zp.apps.dev39.openshift.opentlc.com/#/
    
 5) Import the JSON Notebook in the Zepplin Notebook <br> <br>
    Currently it is in our GitHub URL <br>
    https://raw.githubusercontent.com/Pkrish15/uber-datanalysis/master/Uber%20Data%20Analysis.json <br> <br>
    
 6) Run the Zepplin Notebook at every stages, Please read the zepplin tutorial if required. <br> <br>
 
 # References 
 https://mapr.com/blog/monitoring-real-time-uber-data-using-spark-machine-learning-streaming-and-kafka-api-part-1/ <br>
 
 https://mapr.com/blog/monitoring-real-time-uber-data-using-spark-machine-learning-streaming-and-kafka-api-part-2/ <br>
 
 https://www.youtube.com/watch?v=52zTo7bznXw <br>
 
 https://www.youtube.com/watch?v=nncxYGD6m7E <br>
 
endif::showscript[]



