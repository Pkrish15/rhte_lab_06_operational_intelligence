:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:

= Spark Analysis of Historical Data

.Goals
* Apache Spark Deployment using Apache Zeppelin
* Uber Historical Data Analysis using SparkSQL
* Usage of Interactive NoteBooks like Apache Zeppelin

.Prerequisite
* Skills
** Programming Knowledge of Python, Scala and Java Languages
** Knowledge of git
** Knowledge on OpenShift Deployments
** Basic Knowledge of Cluster Computing and Distributed Architetcures

* Tools
** `curl` utility
** `sed` utility
** `oc` version 3.9 utility
** `Interactive Notebooks - Jupyter and Zeppelin`
** `git` command basics

:numbered:

== Overview

=== Background
 
Apache Spark is a fast, easy-to-use and general engine for big data processing that has built-in modules for streaming, SQL, Machine Learning (ML) and graph processing. Spark is a cluster-computing framework, which means that it competes more with MapReduce than with the entire Hadoop ecosystem. For example, Spark doesn't have its own distributed filesystem, but can use HDFS. Spark uses memory and can use disk for processing, whereas MapReduce is strictly disk-based.

MLlib is Spark's scalable machine learning library consisting of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, as well as underlying optimization primitives.

Apache Zeppelin is a multi-purpose web-based notebook which brings data ingestion, data exploration, visualization, sharing and collaboration features to Hadoop and Spark. Based on the concept of an interpreter that can be bound to any language or data processing backend, Zeppelin is a web based notebook server. As one of its backends, Zeppelin implements Spark, and other implementations, such as Hive, Markdown, D3 etc., are also available. Zeppelin has support for multiple interpreters like Scala, Python, Kotlin, HBase.

The Spark Notebook is the open source notebook aimed at enterprise environments, providing Data Scientists and Data Engineers with an interactive web-based editor that can combine Scala code, SQL queries, Markup and JavaScript in a collaborative manner to explore, analyse and learn from massive data sets.

Spark SQL is a Spark module for structured data processing. It enables unmodified Hadoop Hive queries to run up to 100x faster on existing deployments and data. It also provides powerful integration with the rest of the Spark ecosystem (e.g., integrating SQL query processing with machine learning).

Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. Spark Actions are RDD operations that produce non-RDD values. They materialize a value in a Spark program. In other words, a RDD operation returns a value of any type.

SparkContext is the heart of a Spark application. Once a SparkContext is created you can use it to create RDDs, accumulators and broadcast variables, access Spark services and run jobs (until SparkContext is stopped).

=== Reference
 
=== Github project for the analysis and the use of notebooks

The below link is the GitHub URL for this code. Main.scala is the code applied to the Zeppelin Notebook to analyse the data.
Since we have used SparkSQL, we can keep on querying the data with different results to get different statistics. We can alter the queries and take different views of the data.

https://github.com/gpe-mw-training/operational_intelligence/tree/master/uber-data-analysis

https://raw.githubusercontent.com/gpe-mw-training/operational_intelligence/master/uber-data-analysis/notebooks/Uber.json

Zeppelin has a built in Scala and Python Intepretor which can visualize the data.

== Lab Asset Overview

. Log into OpenShift Environment using OC Client Tool to your Lab Region

-----
$ oc login https://master.$REGION.openshift.opentlc.com -u $OCP_USERNAME -p $OCP_PASSWD
-----

== Lab Instructions
=== Provision Oshinko Cluster - Spark Cluster
. Create Deployment Objects using Template
----
$ oc create -f https://raw.githubusercontent.com/gpe-mw-training/operational_intelligence/master/templates/resources.yaml
...
[root@localhost ~]# oc create -f https://raw.githubusercontent.com/gpe-mw-training/operational_intelligence/master/templates/resources.yaml
serviceaccount "oshinko" created
rolebinding "oshinko-edit" created
configmap "oshinko-py36-conf" created
template "oshinko-python-spark-build-dc" created
template "oshinko-python36-spark-build-dc" created
template "oshinko-java-spark-build-dc" created
template "oshinko-scala-spark-build-dc" created
template "oshinko-webui-secure" created
template "oshinko-webui" created
template "radanalytics-jupyter-notebook" created


[root@localhost ~]# oc new-app oshinko-webui

--> Deploying template "user3-uber-data/oshinko-webui" to project user3-uber-data-history

     * With parameters:
        * SPARK_DEFAULT=
        * OSHINKO_WEB_NAME=oshinko-web
        * OSHINKO_WEB_IMAGE=radanalyticsio/oshinko-webui:stable
        * OSHINKO_WEB_ROUTE_HOSTNAME=
        * OSHINKO_REFRESH_INTERVAL=5

--> Creating resources ...
    service "oshinko-web-proxy" created
    service "oshinko-web" created
    route "oshinko-web" created
    deploymentconfig "oshinko-web" created
--> Success
    Access your application via route 'oshinko-web-user3-uber-data.apps.6d13.openshift.opentlc.com' 
    Run 'oc status' to view your app.

[root@localhost ~]# oc get routes
NAME                HOST/PORT                                                             PATH      SERVICES                            PORT            TERMINATION   WILDCARD
oshinko-web         oshinko-web-user3-uber-data-apps.6d13.openshift.opentlc.com   /webui    oshinko-web(50%),oshinko-web(50%)   <all>                         None
oshinko-web-proxy   oshinko-web-user3-uber-data-apps.6d13.openshift.opentlc.com   /proxy    oshinko-web-proxy                   oc-proxy-port                 None
...
----
=== Oshinko Cluster
The Oshinko project covers several individual applications which all focus on the goal of deploying and managing Apache Spark clusters on Red Hat OpenShift and OpenShift Origin.

With the Oshinko family of applications you can create, scale, and destroy Apache Spark clusters. These clusters can then be used by your applications within an OpenShift project by providing a simple connection URL to the cluster. There are multiple paths to achieving this: browser based graphical interface, command line tool, and a RESTful server.

To begin your exploration, we recommend starting with the oshinko-webui application. The oshinko-webui is a self-contained deployment of the Oshinko technologies. An OpenShift user can deploy the oshinko-webui container into their project and then access the server with a web browser. Through the browser interface you will be able to manage Apache Spark clusters within your project.

Another important part of Oshinko to highlight is the oshinko-s2i repository and associated images which implement the source-to-image workflow for Apache Spark based applications. These images enable you to create full applications that can be built, deployed and upgraded directly from a source repository.

. Access the route url http://oshinko-web-user3-uber-data-apps.6d13.openshift.opentlc.com/webui/#/clusters

Click Deploy Button and to Add the Cluster as shown in the Below figure.

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/oshinkoCluster.png[oshinko]

=== Provision Zeppelin

. Create Deployment Objects using Template
+
-----
$ oc create -f https://raw.githubusercontent.com/gpe-mw-training/operational_intelligence/master/templates/zeppelin-openshift.yaml 

...
template "apache-zeppelin-openshift" created
-----

. Apply the zeppelin template, and the intepreters can be set as a parameters

+
-----
...

oc new-app --template=apache-zeppelin-openshift \
> --param=APPLICATION_NAME=apache-zeppelin \
> --param=GIT_URI=https://github.com/rimolive/zeppelin-openshift.git \
> --param=ZEPPELIN_INTERPRETERS=md 
--> Deploying template "user3-uber-data/apache-zeppelin-openshift" to project user3-uber-data

     * With parameters:
        * Application Name=apache-zeppelin
        * Git Repository URL=https://github.com/rimolive/zeppelin-openshift.git
        * Zeppelin Interpreters=md

--> Creating resources ...
    deploymentconfig "apache-zeppelin" created
    service "apache-zeppelin" created
    service "apache-zeppelin-headless" created
    route "apache-zeppelin" created
    buildconfig "apache-zeppelin" created
    imagestream "apache-zeppelin" created
    imagestream "zeppelin-openshift" created
--> Success
    Access your application via route 'apache-zeppelin-user3-uber-data.apps.6d13.openshift.opentlc.com' 
    Build scheduled, use 'oc logs -f bc/apache-zeppelin' to track its progress.
    Run 'oc status' to view your app.
...
-----
. Get the Routes and Access the URL.
+
-----
...
[root@localhost ~]# oc get routes
NAME                         HOST/PORT                                                                            PATH      SERVICES                            PORT            TERMINATION   WILDCARD
apache-zeppelin              apache-zeppelin-user3-uber-data-apps.6d13.openshift.opentlc.com                        apache-zeppelin                     8080-tcp                      None
oshinko-web                  oshinko-web-user3-uber-data-apps.6d13.openshift.opentlc.com                  /webui    oshinko-web(50%),oshinko-web(50%)   <all>                         None
oshinko-web-proxy            oshinko-web-user3-uber-data-apps.6d13.openshift.opentlc.com                  /proxy    oshinko-web-proxy                   oc-proxy-port                 None
uber-data-cluster-ui-route   uber-data-cluster-ui-route-user3-uber-data-apps.6d13.openshift.opentlc.com             uber-data-cluster-ui                <all>                         None

...
-----

=== Modify Zeppelin Interpreter JVM settings

== What is the Zeppelin Interpreter ?

Zeppelin Interpreter is a plug-in which enables Zeppelin users to use a specific language/data-processing-backend. For example, to use Scala code in Zeppelin, you need %spark interpreter.

When you click the +Create button in the interpreter page, the interpreter drop-down list box will show all the available interpreters on your server.


== What port(s) and protocols is it listening on ?

Below Figure Explain's the configuration of Zeppelin

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/ZeppelinNewSettings.png[portzepp]

-----
...
Please add these properties in the zeppelin Intepreter settings

spark.driver.bindAddress	                            0.0.0.0
spark.driver.host	                                   apache-zeppelin
spark.driver.blockManager.port	                     	42100
spark.driver.port	                                   42000
...
-----
Ensure that you have apache-zeppelin having this kind of configuration as shown in the given below figure.

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/PortZeppelin.png[portzepp]

=== Deployment Topology Diagram

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/DeploymentTopology.png[zeppelinTopo]

=== Importance of ClusterQuota and Limit Range

==== Cluster Quota
A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per project. It can limit the quantity of objects that can be created in a project by type, as well as the total amount of compute resources and storage that may be consumed by resources in that project.

==== Limit Range
A limit range, defined by a LimitRange object, enumerates compute resource constraints in a project at the pod, container, image, image stream, and persistent volume claim level, and specifies the amount of resources that a pod, container, image, image stream, or persistent volume claim can consume.

All resource create and modification requests are evaluated against each LimitRange object in the project. If the resource violates any of the enumerated constraints, then the resource is rejected. If the resource does not set an explicit value, and if the constraint supports a default value, then the default value is applied to the resource.

==== CPU Limits

Each container in a pod can specify the amount of CPU it is limited to use on a node. CPU limits control the maximum amount of CPU that your container may use independent of contention on the node. If a container attempts to exceed the specified limit, the system will throttle the container. This allows the container to have a consistent level of service independent of the number of pods scheduled to the node.

==== Memory Requests
By default, a container is able to consume as much memory on the node as possible. In order to improve placement of pods in the cluster, specify the amount of memory required for a container to run. The scheduler will then take available node memory capacity into account prior to binding your pod to a node. A container is still able to consume as much memory on the node as possible even when specifying a request.

==== Memory Limits
If you specify a memory limit, you can constrain the amount of memory the container can use. For example, if you specify a limit of 200Mi, a container will be limited to using that amount of memory on the node. If the container exceeds the specified memory limit, it will be terminated and potentially restarted dependent upon the container restart policy.



=== Inject Uber historical data into Zeppelin

. Navigate to Storage-->Create Storage. Create a PVC of 50MB from the Create Storage screen. 
+
image::https://github.com/Pkrish15/uber-datanalysis/blob/master/uber-data.png[uber7]

. Attach it to the Pod.
+
image::https://github.com/Pkrish15/uber-datanalysis/blob/master/pvc.png[uber9]

. Mount the Volume as shown below.
+
image::https://github.com/Pkrish15/uber-datanalysis/blob/master/uber-data-pvc.png[uber8]

. Copy the Local Data to the Pod Directory using Rsync Command (Screen shot given below)
+
----
oc rsync src directory pod directory:/data
for Example
oc rsync /home/prakrish/workspace/uberdata-analysis/src/main/resources/data/ apache-zeppelin-2-f89tz:/data 
----
+
image::https://github.com/Pkrish15/uber-datanalysis/blob/master/ocrsync.png[uber10]

. Once the data copied, Open the Zeppelin URL
+
image::https://github.com/Pkrish15/uber-datanalysis/blob/master/zeppelin.png[uberstream7]

. Import the JSON File given the GitHub URL in the Zeppelin Notebook
+
image::https://github.com/Pkrish15/uber-datanalysis/blob/master/UberDataImport.png[uberstream8]

. You can change the directory structure in zeppelin notebook pointing to the data directory in POD
+
image::https://github.com/Pkrish15/uber-datanalysis/blob/master/pvc-data-zeppelin.png[data-placeholder]

. Execute the cell at very stages and you can visualize the data, upon each query
+
image::https://github.com/Pkrish15/uber-datanalysis/blob/master/UberCellAnalysis.png[uberstream9]


== Conclusions

You have learned the concepts of Spark Cluster, Actions, Transformations, Spark SQL and NoteBook Deployment.

== Questions

TO-DO :  questions to test student knowledge of the concepts / learning objectives of this lab

== Appendix

===  Overview 
So far we learned about Spark uses Zeppelin Notebook and Performs the Data Analysis based on the Historical Data.

===  What did we learn?
This Lab helps the students to get to know the basics of interactive notebook usage in the current big data scenario.

Basic deployment of spark jobs on Oshinko cluster amd connectivity of zeppelin notebook to the Spark Oshinko Cluster.

SparkSQL - Excellent API for structured streaming and it is an advanced concept in Apache Spark. Since, it uses catalyst optimizer, it provides an excellent performance benefits and it is the most prefered query language for the datascientists all over the world.

=== Background

According to Gartner, by 2020, a quarter of a billion connected cars will form a major element of the Internet of Things. Connected vehicles are projected to generate 25GB of data per hour, which can be analyzed to provide real-time monitoring and apps, and will lead to new concepts of mobility and vehicle usage. One of the 10 major areas in which big data is currently being used to excellent advantage is in improving cities. For example, the analysis of GPS car data can allow cities to optimize traffic flows based on real-time traffic information.

Uber is using big data to perfect its processes, from calculating Uber’s pricing, to finding the optimal positioning of cars to maximize profits. In this series of blog posts, we are going to use public Uber trip data to discuss building a real-time example for analysis and monitoring of car GPS data. There are typically two phases in machine learning with real-time data:

Data Discovery: The first phase involves analysis on historical data to build the machine learning model.

Analytics Using the Model: The second phase uses the model in production on live events. (Note that Spark does provide some streaming machine learning algorithms, but you still often need to do an analysis of historical data.)building the model.

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/1.jpg[uberstream]


In this first post, I’ll help you get started using Apache Spark’s machine learning K-means algorithm to cluster Uber data based on location.

=== Where do we get these DataSets?

http://data.beta.nyc/dataset/uber-trip-data-foiled-apr-sep-2014 

===  What algorithm is suitable for Data Analytics?

Clustering uses unsupervised algorithms, which do not have the outputs (labeled data) in advance. K-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters (k). Clustering using the K-means algorithm begins by initializing all the coordinates to k number of centroids. With every pass of the algorithm, each point is assigned to its nearest centroid based on some distance metric, which is usually Euclidean distance. The centroids are then updated to be the “centers” of all the points assigned to it in that pass. This repeats until there is a minimum change in the centers.

===  What is in the data set?

The Data Set Schema
Date/Time: The date and time of the Uber pickup
Lat: The latitude of the Uber pickup
Lon: The longitude of the Uber pickup
Base: The TLC base company affiliated with the Uber pickup
​​The Data Records are in CSV format. An example line is shown below:

2014-08-01 00:00:00,40.729,-73.9422,B02598 

===  How do we do it?

Load the data into a Spark Data Frame

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/2.png[uberstream2]

Define Features Array
In order for the features to be used by a machine learning algorithm, the features are transformed and put into Feature Vectors, which are vectors of numbers representing the value for each feature. Below, a VectorAssembler is used to transform and return a new DataFrame with all of the feature columns in a vector column. <br>

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/3.png[uberstream3]

Create a KMeans Object, set the parameters to define the number of clusters and the maximum number of iterations to determine the clusters and then we fit our model to the input data.

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/4.png[uberstream4]

Output, Cluster Centers are displayed on the Google Map

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/5.png[uberstream5]

Further Analysis of cluster

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/6.png[uberstream6]

===  What are the Analysis Questions? 

**** Which hour of the day and which cluster had highest number of pickups?

**** How many pickups occured in each cluster?

== Basic Architecture Diagram of Zeppelin

 Basic Architecture Diagram of Zeppelin will explain on how it works.

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/zeppelinArchitecture.png[zepp]

ifdef::showscript[]



endif::showscript[]
