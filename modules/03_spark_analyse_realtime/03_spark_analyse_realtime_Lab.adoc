:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:

= Spark Analysis of Realtime Data

.Goals
* Apache Spark Deployment and Kafka Streaming.
* Uber Realtime Data Analysis using Advanced Concept of Spark (SparkSQL)
* Usage of Interactive NoteBooks like Apache Zeppelin


.Prerequisite
* Skills
** Don't clean up the Lab1 Environment, As we are going to use the Kafka Cluster to do the Realtime data streaming
** Programming Knowledge of Python, Scala and Java Languages
** Knowledge of GIT Commands
** Knowledge on OpenShift Deployments
** Knowledge of Messaging system and Messaging Architecture
** Basic Knowledge of Cluster Computing and Distributed Architetcures
** GIT Clone of the Source code into Student's Desktop

* Tools
** `curl` utility
** `sed` utility
** `oc` version 3.9 utility
** `Interactive Notebooks - Jupyter and Zeppelin`
** `git` command basics


:numbered:

== Overview

We will build a real-time example for analysis and monitoring of Uber car GPS trip data. 

The first project discussed creating a machine learning model using Apache Sparkâ€™s K-means algorithm to cluster Uber data based on location. This complex project will discuss using the saved K-means model with streaming data to do real-time analysis of where and when Uber cars are clustered

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/picture1.png[uberstream10]

=== Background


=== Reference

== Lab Asset Overview

=== Environment Variables

=== OpenShift access

=== Code Analysis of Lab 3

https://github.com/gpe-mw-training/operational_intelligence/tree/master/uber-kafka-streaming

There are 6 main programs in the GitHub, We can individually deploy as a Spark Job or Can execute in the Zeppelin Notebook.
Entire Flow is with KafkaProducerConsumer.scala. Hence we analyse with this code and apply in the zeppelin notebook.

=== Data Pipeline

A Spark streaming application subscribed to the first topic:

Ingests a stream of uber trip events

Identifies the location cluster corresponding to the latitude and longitude of the uber trip.

Adds the cluster location to the event and publishes the results in JSON format to another topic.

A Spark streaming application subscribed to the second topic:

Analyzes the uber trip location clusters that are popular by date and time.

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/picture2.png[uberstream11]

=== Data Flow in Detail
The example data set is Uber trip data, which you can read more about in part 1 of this series. The incoming data is in CSV format, an example is shown below , with the header:

date/time, latitude,longitude,base
2014-08-01 00:00:00,40.729,-73.9422,B02598

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/picture3.png[uber12]

Data will be enriched are in JSON Format which is given below

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/picture4.png[berstream13]

Spark Kafka Producer Consumer Code with Enriched Data

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/picture5.png[berstream14]

=== Perform these steps on the code or in Zeppelin Notebook

Parse the DataSet Records (Uber case class)
Load the KMeans model
=== Spark Streaming Code

These are the basic steps for the Spark Streaming Consumer Producer code:

Configure Kafka Consumer Producer properties.

Initialize a Spark StreamingContext object. Using this context, create a DStream which reads message from a Topic.

Apply transformations (which create new DStreams).

Write messages from the transformed DStream to a Topic.

Start receiving data and processing. Wait for the processing to be stopped.

We will go through each of these steps with the example application code.

=== Configure Spark Kafka Consumer Producer Properties

The first step is to set the KafkaConsumer and KafkaProducer configuration properties, which will be used later to create a DStream for receiving/sending messages to topics. You need to set the following paramters:


Key and value deserializers: for deserializing the message.

Auto offset reset: to start reading from the earliest or latest message.

Bootstrap servers: this can be set to a dummy host:port since the broker address is Strimzi Kafka POD


----
...

[root@localhost ~]# oc login -u user5 -p r3dh4t1! https://master.6d13.openshift.opentlc.com/
Login successful.

You have one project on this server: "uber-realtimedata-analysis-user5"

Using project "uber-data-user5".
[root@localhost ~]# oc get routes
NAME              HOST/PORT                                                         PATH      SERVICES          PORT       TERMINATION   WILDCARD
apache-zeppelin   apache-zeppelin-uber-realtimedata-analysis-user5.apps.6d13.openshift.opentlc.com             apache-zeppelin   8080-tcp                 None
...
----

== Zeppelin Configuration Changes to run the code

This is one of the most critical steps, Please follow the screen shot's carefully. Missing a single step will lead to unexpected results and exceptions.

----
...
$ oc get pods

$ oc rsh apache-zeppelin-2-dr8s6

sh-4.2$ cd /opt/zeppelin/conf/

sh-4.2$ ls
configuration.xsl  log4j_yarn_cluster.properties  zeppelin-site.xml
interpreter-list   shiro.ini.template		  zeppelin-site.xml.template
interpreter.json   zeppelin-env.cmd.template
log4j.properties   zeppelin-env.sh.template

sh-4.2$ mv zeppelin-env.sh template zeppelin-env.sh

sh-4.2$ vi zeppelin-env.sh

export SPARK_SUBMIT_OPTIONS="--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0"

sh-4.2$ esc+wq!

...
----
== Zeppelin UI Changes to Run the Code

Make Changes in Spark.Memory Parameters to 5G

zeppelin Dependency Local Repo as shown in the Figure

image::https://github.com/Pkrish15/uber-datanalysis/blob/master/ZeppelinUIChangesLab3.png[zepp5ui]


== Conclusions

Finally you have learned the concepts of Spark Cluster, Actions, Transformations, Spark SQL and NoteBook Deployment.


== Questions

TO-DO :  questions to test student knowledge of the concepts / learning objectives of this lab

== Appendix

===  Overview 

So far we learned about Spark uses Zeppelin Notebook and Performs the Data Analysis based on the Uber RealTime Data.


ifdef::showscript[]

endif::showscript[]
