:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:

= Spark Real Time Analysis of Uber Data

.Prerequisite
* Skills
** Programming Knowledge of Python, Scala and Java Languages
** Knowledge of git
** Knowledge on OpenShift Deployments
** Basic Knowledge of Cluster Computing and Distributed Architectures

* Tools
** `curl` utility
** `sed` utility
** `oc` version 3.9 utility
** `Interactive Notebooks - Apache Zeppelin`
** `git` command basics

:numbered:

== Overview

=== Lab Goals

. Kafka streaming of Uber.csv file to Apache Spark on Oshinko Cluster.
. Large Data Analysis using Apache Spark with Apache Kafka.

=== Technical Background

==== Apache Spark with Red Hat Strimzi Cluster
. *Apache Strimzi*


=== Reference
.  https://kafka.apache.org/
.  https://strimzi.io/

=== Validate Environment Variables
In a previous lab, you should have already set various environment variables in the shell of your lab environment.

At this time, ensure that the following environment variables remain set:

-----
echo "export OCP_PROJECT=\$OCP_USERNAME-uber-realtime-data" >> ~/.bashrc

echo "export OCP_REGION=`echo $HOSTNAME | cut -d'.' -f2`" >> ~/.bashrc

echo "export OCP_DOMAIN=clientvm.\$OCP_REGION.rhte.opentlc.com" >> ~/.bashrc

echo "export OCP_WILDCARD_DOMAIN=apps.\$OCP_DOMAIN" >> ~/.bashrc

source ~/.bashrc

$ echo $OCP_USERNAME
developer

$ echo $OCP_PASSWD
xxxxxxxx
-----

== Lab Asset Overview

This lab provides a set of assets to assist with the provisioning of Oshinko and Zeppelin.
You will want to clone these lab assets to your lab environment so that you can review them.

. Make a new directory where all lab assets will reside.
  Already the lab assets are cloned in Lab1. Please refer instructions of Lab1.
. Change directory to the newly cloned project.
+
-----
$ cd $HOME/lab/operational_intelligence/
-----

. Review the various files specific to this lab :
+
-----
uber-realtime-spark-stream-analysis/
├── data
│   ├── cluster.txt
│   └── uber.csv
├── notebooks
│   ├── Spark ML KMeans - Uber.json
│   ├── Spark ML KMeansUber.json
│   ├── uberml201.json
│   ├── uberstreaming201.json
│   ├── uberstreaming2.json
│   └── uberZeppelinnotebook.json
├── pom.xml
├── ReadMe.adoc
└── src
    └── main
        ├── java
        │   └── com
        │       └── streamskafka
        │           └── uber
        │               ├── MsgConsumer.java
        │               └── MsgProducer.java
        └── scala
            └── com
                ├── sparkkafka
                │   └── uber
                │       ├── JSONUtil.scala
                │       ├── KafkaProducerExample.scala
                │       ├── SparkKafkaConsumerProducer.scala
                │       └── SparkKafkaConsumer.scala
                └── sparkml
                    └── uber
                        ├── ClusterUber.scala
                        └── SqlUber.scala

-----


. Several key assets to review are as follows:

.. *pom.xml*
+
Notice that community Apache Spark and community Scala packages are being utilized.
At this time, Red Hat does not intend to provide supported versions of these packages.

.. *Raw Uber historical Data*

... Available in the lab assets at:  `uber-data-analysis/src/main/resources/data/uber.csv`
... It is the raw data from the UberData for NLC which describes the Latitude, Longitude, timestamp and BaseId

.. *SparkKafkaConsumerProducer.scala*
+
SparkkafkaConsumerProducer.scala is the code applied to the Zeppelin Notebook to analyze the data.

... This file is provided to you as background and context only.
For the purpose of this lab, you will not be modifying or compiling this scala class file.
... Compiling the SparkkafkaConsumerProducer.scala has already been done for you and the output being the exported asset introduced next:  _Uber.json_.

.. *Uber.json*
+
... Later in this lab you will analyze this Uber.json.  It is generated from SCALA IDE or Using Zeppelin Paragraph Cells Compilation.
... Later in this lab you will import this JSON File in the Zeppelin NoteBook and run the each paragraph .

.. *templates*

... *oshinko-cluster.yaml*
....  This template file use to create the deployment Objects of Oshinko Cluster and the students are expected to run for provisioning commands. Below given a detailed explaination of Oshinko Cluster provisioning.

... *zeppelin-openshift.yaml*

.... This template file use to create the deployment objects of Zeppelin and the students are expected to run for provisioning commands. Below given a detailed explaination of Zeppelin Provisioning and its interpreter Configurations.


== Oshinko


=== Oshinko Web UI

. Log into OpenShift Environment using OC Client Tool to your Lab Region
+
-----
$ oc login https://$HOSTNAME:8443 -u $OCP_USERNAME -p $OCP_PASSWD
-----

. Create and switch to the OCP project specific to this lab:
+
-----
$ oc new-project $OCP_USERNAME-uber-realtime-data --description=$OCP_USERNAME-uber-realtime-data



$ oc project $OCP_USERNAME-uber-realtime-data
-----

. In your OpenShift namespace, create needed Oshinko templates:
+
-----
$ oc create \
     -f https://raw.githubusercontent.com/gpe-mw-training/operational_intelligence/1.0.3/templates/oshinko-cluster.yaml \
     -n $OCP_USERNAME-uber-realtime-data
-----

. Provision the Oshinko-WebUI
+
-----

$ oc new-app oshinko-webui -n $OCP_USERNAME-uber-realtime-data > /tmp/oshinko-web.txt

-----
+
.. Review the output found in /tmp/oshinko-web.txt
+
----
--> Deploying template "user3-uber-realtime-data/oshinko-webui" to project user3-uber-realtime-data

     * With parameters:
        * SPARK_DEFAULT=
        * OSHINKO_WEB_NAME=oshinko-web
        * OSHINKO_WEB_IMAGE=radanalyticsio/oshinko-webui:stable
        * OSHINKO_WEB_ROUTE_HOSTNAME=
        * OSHINKO_REFRESH_INTERVAL=5

--> Creating resources ...
    service "oshinko-web-proxy" created
    service "oshinko-web" created
    route "oshinko-web" created
    deploymentconfig "oshinko-web" created
--> Success
    Access your application via route 'oshinko-web-user3-uber-data.apps.6d13.openshift.opentlc.com'
    Run 'oc status' to view your app.

----
. Review the template that has been created
+
-----
$ oc get template oshinko-webui -n $OCP_USERNAME-uber-realtime-data -o yaml | more
-----


. Wait until both containers of the oshinko-web pod have started:
+
-----
$ oc get pods -w
NAME                  READY     STATUS    RESTARTS   AGE


oshinko-web-1-86blg   2/2       Running   0
-----


. Log into the Oshinko web UI
.. Point your browser to the output of the following command:
+
-----
$ echo -en "\n\nhttp://"$(oc get route/oshinko-web -o template --template {{.spec.host}} -n $OCP_USERNAME-uber-realtime-data)/webui"\n\n"
-----
+
image::images/oshinko_homepage.png[]

.. At this time, the Oshinko web UI is not secured. It is recommended to use Oshinko webui non-secured port.
+
Subsequently, you should be able to access the UI without authenticating.

=== Oshinko Cluster
In a real-time scenario, we would have different clusters for each services and use cases. Hence, we are not going to use the same cluster which was used in Lab2. We are going to create a new cluster for Lab3.
Because of this cluster can act as an Independent services. Hence we are doing this.

Via the Oshinko Web UI, you can now create an Oshinko cluster which will consist of both a master and worker pods.

. In the homepage of the Oshinko Web UI, click: `Deploy`
. Populate the pop-up with the following:
.. *Name*: uber-realtime-data-cluster
.. *Number of workers*: 1
. Click the `Deploy` button
+
image::images/provision_oc.png[]

. Two `Deployment Config` resources will have been created.
These two DCs are responsible for the provisioning of the Oshinko Master and Worker.
These DCs are not configured with limits and requests as required by your lab environment.
Execute the following series of steps to add limits and requests to your DCs so that the underlying pods will start:

.. Add limits and requests to the master pod:
+
-----
$ oc patch dc/uber-realtime-data-cluster-m -n $OCP_USERNAME-uber-realtime-data \
    --patch '{"spec":{"strategy":{"resources": { "limits":{"cpu": "2","memory": "4Gi"},"requests":{"cpu":"1","memory":"512Mi"}   } }}}'

$ oc patch dc/uber-realtime-data-cluster-m -n $OCP_USERNAME-uber-realtime-data \
    --patch '{"spec":{"template":{"spec":{"containers":[{"name":"uber-realtime-data-cluster-m", "resources": {   "limits":{"cpu": "1","memory": "2Gi"},"requests":{"cpu":"500m","memory":"256Mi"}   }}]}}}}'
-----

.. Add limits and requests to the worker pod:
+
-----
$ oc patch dc/uber-realtime-data-cluster-w -n $OCP_USERNAME-uber-realtime-data \
   --patch '{"spec":{"strategy":{"resources": { "limits":{"cpu": "2","memory": "4Gi"},"requests":{"cpu":"1","memory":"512Mi"}   } }}}'


$ oc patch dc/uber-realtime-data-cluster-w -n $OCP_USERNAME-uber-realtime-data \
       --patch '{"spec":{"template":{"spec":{"containers":[{"name":"uber-realtime-data-cluster-w", "resources": {   "limits":{"cpu": "1","memory": "2Gi"},"requests":{"cpu":"500m","memory":"256Mi"}   }}]}}}}'
-----

.. The end result are the new Oshinko master and worker pods having started:
+
-----
$ oc get pods
NAME                          READY     STATUS      RESTARTS   AGE
...

uber-realtime-data-cluster-m-2-b4cl4   1/1       Running     0          1m
uber-realtime-data-cluster-w-2-678fp   1/1       Running     0          10s
-----


== Zeppelin

=== Overview
A completely open web-based notebook that enables interactive data analytics. Apache Zeppelin is a new and incubating multi-purposed web-based notebook which brings data ingestion, data exploration, visualization, sharing and collaboration features to Hadoop and Spark.

Interactive browser-based notebooks enable data engineers, data analysts and data scientists to be more productive by developing, organizing, executing, and sharing data code and visualizing results without referring to the command line or needing the cluster details. Notebooks allow these users not only allow to execute but to interactively work with long workflows.  There are a number of notebooks available with Spark. iPython remains a mature choice and great example of a data science notebook.  The Hortonworks Gallery provides an Ambari stack definition to help our customers quickly set up iPython on their Hadoop clusters.

Apache Zeppelin is a new and upcoming web-based notebook which brings data exploration, visualization, sharing and collaboration features to Spark.   It support Python, but also a growing list of programming languages such as Scala, Hive, SparkSQL, shell and markdown.

=== Provision

. If you are not already there, switch to the OCP project specific to this lab:
+
-----
$ oc project $OCP_USERNAME-uber-realtime-data
-----

. In your OpenShift namespace, create the needed zeppelin templates:
+
-----
$ oc create \
     -f https://raw.githubusercontent.com/gpe-mw-training/operational_intelligence/1.0.4/templates/zeppelin-openshift.yaml \
     -n $OCP_USERNAME-uber-realtime-data


     ...
     template "apache-zeppelin-openshift" created

-----

. Review the templates that have been created:
+
-----
$ oc get templates -n $OCP_USERNAME-uber-realtime-data


-----

. Provision the Zeppelin web-ui
+
-----
$ oc new-app --template=apache-zeppelin-openshift \
  --param=APPLICATION_NAME=apache-zeppelin \
  --param=GIT_URI=https://github.com/rimolive/zeppelin-openshift.git \
  --param=ZEPPELIN_INTERPRETERS=md \
  >> /tmp/zeppelin-web.txt
-----
.. Review the output found in `/tmp/zeppelin-web.txt`
+
-----

....

--> Deploying template "user3-uber-realtime-data/apache-zeppelin-openshift" to project user3-uber-realtime-data

     * With parameters:
        * Application Name=apache-zeppelin
        * Git Repository URL=https://github.com/rimolive/zeppelin-openshift.git
        * Zeppelin Interpreters=md

--> Creating resources ...
    deploymentconfig "apache-zeppelin" created
    service "apache-zeppelin" created
    service "apache-zeppelin-headless" created
    route "apache-zeppelin" created
    buildconfig "apache-zeppelin" created
    imagestream "apache-zeppelin" created
    imagestream "zeppelin-openshift" created
--> Success
    Access your application via route 'apache-zeppelin-user3-uber-realtime-data.apps.6d13.openshift.opentlc.com'
    Build scheduled, use 'oc logs -f bc/apache-zeppelin' to track its progress.
    Run 'oc status' to view your app.


-----

.. Expect that a apache-zeppelin build pod run to completion from which the apache-zeppelin deployment will start:
+
-----
$ oc get pods -w


NAME                      READY     STATUS      RESTARTS   AGE
apache-zeppelin-1-build   0/1       Completed   0          2m
apache-zeppelin-1-tf9g8   1/1       Running     0          36s

-----

==== Login into Zeppelin UI

Navigate your browser to the output of the following URL:

-----
$ echo -en "\n\nhttp://"$(oc get route/apache-zeppelin -o template --template {{.spec.host}} -n $OCP_USERNAME-uber-realtime-data)/"\n\n"
-----

image::images/zeppelin.png[uberstream7]

== Zeppelin Interpreter

=== Inject Uber RealTime data

In this section of the lab, the raw uber related data found in your lab assets will be mounted to your Apache Zeppelin interpreter.
By keeping the data local to the Zeppelin interpreter, the data analysis will execute quickly.

. Create a configuration map based on the uber.csv data file found in your lab assets:
+
-----
$ oc create configmap uber-realtime-data-cm \
    --from-file=$HOME/lab/operational_intelligence/uber-realtime-data-analysis/src/main/resources/data/uber.csv
-----

. Mount the config map to the `apache-zeppelin` deployment config as a volume:
+
-----
$ oc set volume dc/apache-zeppelin \
         --add --overwrite \
         --name=uber-realtime-data-volume \
         -t configmap \
         --configmap-name=uber-realtime-data-cm \
         -m /data/uber.csv \
         --sub-path=uber.csv \
         --default-mode=0644
-----

. Mount the same config map to the Spark worker deployment config as a volume:
+
-----
$ oc set volume dc/uber-realtime-data-cluster-w  \
         --add --overwrite \
         --name=uber-realtime-data-volume \
         -t configmap \
         --configmap-name=uber-realtime-data-cm \
         -m /data/uber.csv \
         --sub-path=uber.csv \
         --default-mode=0644
-----


=== Port(s) and protocols

The Spark Master and Worker pods need to be configured to communicate with each other along with the Zeppelin Interpreter.

In this section you will use the Zeppelin UI to make these configuration changes.

. Ensure that you have apache-zeppelin having this kind of configuration as shown in the given below figure.
+
----
$ oc get services apache-zeppelin

NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                        AGE
apache-zeppelin   ClusterIP   172.30.43.201   <none>        8080/TCP,42000/TCP,42100/TCP   4m

----
+
Notice the Service 42000 has been exposed to spark.driver.port and 42100 has been exposed spark.driver.blockManager.port

. In the Zeppelin UI, open the Zeppelin UI by navigating to the drop down at the top right corner.
. From the drop down, select:  `Interpreter`.
. Scroll down to the `Spark` section and you should see configurations similar to the following:
+
image::images/ZeppelinNewSettings.png[]


. Make the following changes in this Spark section:

.. Find the existing text box for the URL to the `master` and populate it as follows:
+
-----
master                         : spark://uber-realtime-data-cluster:7077
-----

.. Scroll down to the bottom of the Spark section and add the following new text fields:

... *spark.driver.bindAddress*       :  0.0.0.0
... *spark.driver.host*              :	apache-zeppelin
... *spark.driver.blockManager.port* :	42100
... *spark.driver.port*	             :  42000

. After making the changes, scroll to the bottom of the page and click: `Save`.

. Deployment Topology given below.
+
image::images/DeploymentTopologyLab3.png[DeploymentTopo]

. Deployment Topology with DataFlow.
+
image::images/DeploymentTopologyLab3Data.png[DeploymentTopoData]

. DataPipeLine Architecture Diagram.

Uber trip data is published to a Kafka Streams topic using the Kafka API.
A Spark streaming application subscribed to the first topic:
Ingests a stream of uber trip events
Identifies the location cluster corresponding to the latitude and longitude of the uber trip
Adds the cluster location to the event and publishes the results in JSON format to another topic
A Spark streaming application subscribed to the second topic:
Analyzes the uber trip location clusters that are popular by date and time.


image::images/DataFlowSchematic.png[DataFlow]

=== Zeppelin Kafka Configuration

=== What we did in the Lab2?
In Lab2 we just created a model with the Historical data (Uber.csv), build a training set, Identified the patterns and did a Test Predictions.

In Lab3 we are going to use the Deployed Model and give accurate predictions.

image::images/picture1.png[recall]

=== Creation of Uber NoteBook

Recall that the lab assets provided as SparkProducerConsumer.scala.

This application is currently running on Zeppelin Notebook. Let us explore what it does.

... Incoming Data is in CSV format, get enriched with JSON Format and the KMeans Cluster ID is assigned to it.
+
image::images/picture3.png[LoadData]

+
image::images/picture4.png[KMeans]

+
image::images/picture5.png[KMeansClusterID]

... Parsing the Data Set Records
A Scala Uber case class defines the schema corresponding to the CSV records. The parseUber function parses the comma separated values into the Uber case class.

+
image::images/picture6.png[ubercaseclass]

... Loading the K-Means Model
The Spark KMeansModel class is used to load the saved K-means model fitted on the historical Uber trip data.

+
image::images/picture7.png[LoadKMeans]

Output of Model clusterCenters

+
image::images/picture8.png[outputmodelclusters]

... Next, we create a KMeans object, set the parameters to define the number of clusters and the maximum number of iterations to determine the clusters, and then we fit the model to the input data
+
image::images/KMeansM.png[FeatureArray]

... Next, we use the model to get the clusters for test data in order to further analyze the clustering.
+
image::images/Predictions.png[Predictions]

=== Import Uber _NoteBook_

It is the Source code and Output file that every students expected to Run and visualize the results.

.. Once the data copied, Open the Zeppelin URL using the URL determined in the previous section:
+
-----
$ echo -en "\n\nhttp://"$(oc get route/apache-zeppelin -o template --template {{.spec.host}} -n $OCP_USERNAME-uber-realtime-data)/"\n\n"
-----

.. Import the JSON File given the GitHub URL in the Zeppelin Notebook

.  https://raw.githubusercontent.com/gpe-mw-training/operational_intelligence/master/uber-realtime-data-analysis/notebooks/Uber.json

+
image::images/UberImport.png[uberstream8]

.. You can change the directory structure in zeppelin notebook pointing to the data directory in POD.
   You can Edit the Zeppelin Paragraph and change the Directory Structure to */data/uber.csv*.
   All Paragraphs are editable in Zeppelin Notebook.
.. *Why we are doing this?*
   We need to import the data into the Spark Master Node into the Mounted Volume /data/uber.csv which locates the file to get loaded into the Memory and also it gives the advantage of Data Locality Principle.
+
image::images/ReadCSV.png[ReadCSV]
+
image::images/pvc-data-zeppelin.png[data-placeholder]

=== Execute Uber _Paragraphs_

.. *Spark Streaming Code*

These are the basic steps for the Spark Streaming Consumer Producer code:

... *Configure Kafka Consumer Producer properties*
.. Initialize a Spark StreamingContext object. Using this context, create a DStream which reads message from a Topic.
.. Apply transformations (which create new DStreams).
.. Write messages from the transformed DStream to a Topic.
.. Start receiving data and processing. Wait for the processing to be stopped.
.. We will go through each of these steps with the example application code.

... *Configure Kafka Consumer Producer properties*
.. The first step is to set the KafkaConsumer and KafkaProducer configuration properties, which will be used later to create a DStream for receiving/sending messages to topics. You need to set the following paramters:

.. Key and value deserializers: for deserializing the message.
.. Auto offset reset: to start reading from the earliest or latest message.
.. Bootstrap servers: this can be set to a dummy host:port since the broker address is not actually used by MapR Streams.

... *Initialize a Spark StreamingContext Object*

ConsumerStrategies.Subscribe, as shown below, is used to set the topics and Kafka configuration parameters. We use the KafkaUtils createDirectStream method with a StreamingContext, the consumer and location strategies, to create an input stream from a MapR Streams topic. This creates a DStream that represents the stream of incoming data, where each message is a key value pair. We use the DStream map transformation to create a DStream with the message values.

+
image::images/picture11.png[stream]

+
image::images/picture12.png[DStream]

.. Apply Transformations (Which create New DStreams)

We use the DStream foreachRDD method to apply processing to each RDD in this DStream. We parse the message values into Uber objects, with the map operation on the DStream. Then we convert the RDD to a DataFrame, which allows you to use DataFrames and SQL operations on streaming data.

+
image::images/picture13.png[code]

Here is the Output from the df.show

+
image::images/picture14.png[dfshow]


.. A VectorAssembler is used to transform and return a new DataFrame with the latitude and longitude feature columns in a vector column.

+
image::images/picture15.png[vectorAssemblerLoad]

+
image::images/picture16.png[VectorAssemblerCode]

.. Then the model is used to get the clusters from the features with the model transform method, which returns a DataFrame with the cluster predictions.

+
image::images/picture17.png[LoadModel]


.. Write messages from the transformed DStream to a Topic

+
image::images/picture20.png[Enriched]

.. The Dataset result of the query is converted to JSON RDD Strings, then the RDD sendToKafka method is used to send the JSON key-value messages to a topic (the key is null in this case).

+
image::images/picture21.png[jsonconversion]

+
image::images/picture22.png[dstreamRDD]


.. Start receiving the data and processing it. Wait for the process to be stopped.
To start receiving data, we must explicitly call start() on the StreamingContext, then call awaitTermination to wait for the streaming computation to finish.

+
image::images/picture23.png[processingdata]

.. *Spark Kafka Consumer Code*
Next, we will go over some of the Spark streaming code which consumes the JSON-enriched messages.

+
image::images/picture24.png[consumer]

... *Below is the code for*

.. Creating a Direct Kafka Stream
.. Converting the JSON message values to Dataset[Row] using spark.read.json with the schema
.. Creating two temporary views for subsequent SQL queries
.. Using ssc.remember to cache data for queries

+
image::images/picture26.png[kafkaStream]

.. Output of the Cluster Centers

+
image::images/picture28.png[outputcluster]



== Conclusions

====  What did we learn?

Apache Strimzi - Basics of Apache Strimzi and it's deployment on OpenShift.

Spark Streaming - Excellent API for structured streaming and it is an advanced concept in Apache Spark. Since, it uses catalyst optimizer, it provides an excellent performance benefits and it is the most prefered query language for the datascientists all over the world.

Kafka with Apache Spark Integration - We learned Apache Kafka integration with Spark on Zeppelin Notebook.

== Questions

TO-DO :  questions to test student knowledge of the concepts / learning objectives of this lab

== Appendix


=== Background

According to Gartner, by 2020, a quarter of a billion connected cars will form a major element of the Internet of Things. Connected vehicles are projected to generate 25GB of data per hour, which can be analyzed to provide real-time monitoring and apps, and will lead to new concepts of mobility and vehicle usage. One of the 10 major areas in which big data is currently being used to excellent advantage is in improving cities. For example, the analysis of GPS car data can allow cities to optimize traffic flows based on real-time traffic information.

Uber is using big data to perfect its processes, from calculating Uber’s pricing, to finding the optimal positioning of cars to maximize profits. In this series of blog posts, we are going to use public Uber trip data to discuss building a real-time example for analysis and monitoring of car GPS data. There are typically two phases in machine learning with real-time data:

Data Discovery: The first phase involves analysis on historical data to build the machine learning model.

Analytics Using the Model: The second phase uses the model in production on live events. (Note that Spark does provide some streaming machine learning algorithms, but you still often need to do an analysis of historical data.)building the model.

image:images/1.jpg[uberstream]

In this first post, I’ll help you get started using Apache Spark’s machine learning K-means algorithm to cluster Uber data based on location.

=== Where do we get these DataSets?

http://data.beta.nyc/dataset/uber-trip-data-foiled-apr-sep-2014


ifdef::showscript[]
