:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:

= Kafka ingest and publish lab

.Goals
* Create a Kafka publisher and consumer on a pre-built Kafka deployment
* Create a Kafka connector configured for reading the Uber data file
* Verify that the contents are published on the configured topic using a MsgConsumer

.Prerequisite
* Skills
** 
** 
* Tools
** `curl` utility
** `oc` version 3.9 utility

:numbered:

== Overview

This lab implements the Kafka producer and consumer scenario.

=== Reference

. Kafka on Openshift:
.. link:https://strimzi.io[Project]
+
Kafka on Openshift functionality is provided by the Strimzi project (link above).

== Lab Asset Overview

Your lab environment consists of the following:

. *Remote Virtual Machine*
+
Accessible via the ssh protocol.
It is pre-installed with _OpenShift Container Platform_.


== Order VM via `GUID Grabber`

This section of the lab explains how to access the Red Hat Tech Exchange _GuidGrabber_ in order to obtain a GUID.
This GUID will be used to access the lab environment.

. Begin by going to http://bit.ly/rhte-guidgrabber
+
image::images/gg1.png[GuidGrabber Landing Page]

. From this page select the *Lab Code* :  _A1012_

. Enter the *Activation Key* provided by the lab proctor.

. Click *Next*.

. The resulting page will display your lab's GUID and other useful information about your lab environment.
+
image::images/guid_grabber_response.png[Information Page]

. Your remote virtual machine is accessible via the `ssh` protocol.
+
Follow the directions exactly as indicated in the Guid Grabber Information Page to ssh into your remote lab VM.

. When you are completely done with your lab environment, please click *Reset Workstation* so that you can move on to the next lab.
If you fail to do this, you will be locked into the GUID from the previous lab.
+
[NOTE]
Clicking *Reset Workstation* will not stop or delete the lab environment.


=== Environment Variables

Once you ssh'd into your remote terminal window, you'll want to set the following environment variables:

-----
######  Instructor will provide the values to these environment variables #######

#  Using the above variables, copy & paste the following in the same terminal #

echo "export LAB_CODE=a1012" >> ~/.bashrc
echo "export OCP_USERNAME=developer" >> ~/.bashrc
echo "export OCP_PASSWD=r3dh4t1\!" >> ~/.bashrc
echo "export OCP_REGION=`echo $HOSTNAME | cut -d'.' -f2`" >> ~/.bashrc
echo "export OCP_DOMAIN=clientvm.\$OCP_REGION.rhte.opentlc.com" >> ~/.bashrc
echo "export OCP_WILDCARD_DOMAIN=apps.\$OCP_DOMAIN" >> ~/.bashrc

source ~/.bashrc

#  Using above variables, copy & paste the following in same terminal #
$ export OCP_PROJECT=kafka-project
-----


== Utilities and resources

. Validate that the following exists in the $PATH of the remote virtual machine:

. _git_
. _curl_
. _sed_
. _oc_

. Validate that your virtual machine consists of 16GB RAM and 4 CPUs.
.. Execute:
+
-----
$ cat /proc/meminfo | grep MemTotal

MemTotal:        16016680 kB
-----

.. Execute:
+
-----
$ cat /proc/cpuinfo | awk '/^processor/{print $3}' | wc -l

4
-----


=== Kafka access

Your lab environment includes access to a Kafka installation. To find your environment GUID, refer to https://github.com/RedHatDemos/RHTE-2018/blob/master/GG/gg-dedicated.adoc. The GUID Grabber response will also specify a username of lab-user and the password. Once you have the GUID, you can navigate to the master at https://master.{$GUID}.openshift.opentlc.com.

=== OpenShift Container Platform

You lab environment is built on Red Hat's OpenShift Container Platform.

Access to your OCP resources can be gained via both the `oc` utility as well as the OCP web console.

. Verify that OCP has started:
+
-----
$ sudo systemctl status oc-cluster

...

Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: Server Information ...
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: OpenShift server started.
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: The server is accessible via web console at:
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: https://clientvm.a4f6.rhte.opentlc.com:8443
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: You are logged in as:
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: User:     developer
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: Password: <any value>
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: To login as administrator:
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com occlusterup[20544]: oc login -u system:admin
Aug 31 21:58:27 clientvm.a4f6.rhte.opentlc.com systemd[1]: Started OpenShift oc cluster up Service.
-----

. Using the `oc` utility, log into OpenShift
+
-----
$ oc login https://$HOSTNAME:8443 -u $OCP_USERNAME -p $OCP_PASSWD
-----

. Ensure that your `oc` client is the same minor release version as the server:
+
-----
$ oc version

oc v3.9.30
kubernetes v1.9.1+a0ce1bc657
features: Basic-Auth GSSAPI Kerberos SPNEGO

Server https://master.a4ec.openshift.opentlc.com:443
openshift v3.9.31
kubernetes v1.9.1+a0ce1bc657
-----

.. In the above example, notice that version of the `oc` client is of the same minor release (v3.9.30) of the OpenShift server (v3.9.31)
.. There a known subtle problems with using a version of the `oc` client that is different from your target OpenShift server.

. View existing projects:
+
-----
$ oc get projects

... 

user2-kafka-project                                     Active
-----

. Switch to your  OpenShift project
+
-----
$ oc project $OCP_PROJECT
-----

. Log into OpenShift Web Console
.. Many OpenShift related tasks found in this lab can be completed in the Web Console (as an alternative to using the `oc` utility`.
.. To access, point to your browser to the output of the following:
+
-----
$ echo -en "\n\nhttps://master.$OCP_DOMAIN:8443\n\n"
-----

.. Authenticate using the values of $OCP_USERNAME and $OCP_PASSWD

== Lab instructions

.. Clone the github repo
+
-----
$ git clone https:https://github.com/gpe-mw-training/rhte2018.git
$ cd rhte2018
-----

=== Deploy Kafka producer and consumer

. Create the deployment.yaml in a local directory with deployment sections for the producer and consumer:

[source,text]
----
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: kafka-producer
  name: kafka-producer
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: kafka-producer
    spec:
      containers:
      - name: kafka-producer
        image: strimzici/hello-world-producer:support-training
        env:
          - name: CA_CRT
            valueFrom:
              secretKeyRef:
                name: my-cluster-cluster-ca-cert
                key: ca.crt
          - name: USER_CRT
            valueFrom:
              secretKeyRef:
                name: my-user
                key: user.crt
          - name: USER_KEY
            valueFrom:
              secretKeyRef:
                name: my-user
                key: user.key
          - name: BOOTSTRAP_SERVERS
            value: my-cluster-kafka-bootstrap:9093
          - name: TOPIC
            value: my-topic
          - name: DELAY_MS
            value: "1000"
          - name: LOG_LEVEL
            value: "INFO"
          - name: MESSAGE_COUNT
            value: "1000"
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: kafka-consumer
  name: kafka-consumer
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: kafka-consumer
    spec:
      containers:
      - name: kafka-consumer
        image: strimzici/hello-world-consumer:support-training
        env:
          - name: CA_CRT
            valueFrom:
              secretKeyRef:
                name: my-cluster-cluster-ca-cert
                key: ca.crt
          - name: USER_CRT
            valueFrom:
              secretKeyRef:
                name: my-user
                key: user.crt
          - name: USER_KEY
            valueFrom:
              secretKeyRef:
                name: my-user
                key: user.key
          - name: BOOTSTRAP_SERVERS
            value: my-cluster-kafka-bootstrap:9093
          - name: TOPIC
            value: my-topic
          - name: GROUP_ID
            value: my-group
          - name: LOG_LEVEL
            value: "INFO"
          - name: MESSAGE_COUNT
            value: "1000"
----

. Deploy the Kafka producer and consumer

[source,text]
----
% oc apply -f <path_to_your_local_dir>/deployment.yaml
----

. Check the producer and consumer logs to verify that they are working

[source,text]
----
oc logs $(oc get pod -l app=kafka-producer -o=jsonpath='{.items[0].metadata.name}') -f
oc logs $(oc get pod -l app=kafka-consumer -o=jsonpath='{.items[0].metadata.name}') -f
----

. Add a consumer by creating a deployment yaml file similar to the existing one. The initial section of it should look similar to

[source,text]
----
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: kafka-consumer-2
  name: kafka-consumer-2
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: kafka-consumer-2
    spec:
      containers:
      - name: kafka-consumer-2
        image: strimzici/hello-world-consumer:support-training
----

. Deploy the new consumer

[source,text]
----
% oc apply -f deployment-new.yaml
----

. Observe that the new consumer now receives from one or more partitions that are distinct from the kafka-consumer. The output in their respective logs will look similar to:

[source,text]
----
2018-08-30 18:31:58 INFO  KafkaConsumerExample:24 - Received message:
2018-08-30 18:31:58 INFO  KafkaConsumerExample:25 - 	     partition: 0
2018-08-30 18:31:58 INFO  KafkaConsumerExample:26 - 	     offset: 137772
2018-08-30 18:31:58 INFO  KafkaConsumerExample:27 - 	     value: Hello world - 103
2018-08-30 18:32:00 INFO  KafkaConsumerExample:24 - Received message:
2018-08-30 18:32:00 INFO  KafkaConsumerExample:25 - 	     partition: 1
2018-08-30 18:32:00 INFO  KafkaConsumerExample:26 - 	     offset: 137766
2018-08-30 18:32:00 INFO  KafkaConsumerExample:27 - 	     value: Hello world - 105
2018-08-30 18:32:01 INFO  KafkaConsumerExample:24 - Received message:
2018-08-30 18:32:01 INFO  KafkaConsumerExample:25 - 	     partition: 0
2018-08-30 18:32:01 INFO  KafkaConsumerExample:26 - 	     offset: 137773
2018-08-30 18:32:01 INFO  KafkaConsumerExample:27 - 	     value: Hello world - 106
2018-08-30 18:32:03 INFO  KafkaConsumerExample:24 - Received message:
2018-08-30 18:32:03 INFO  KafkaConsumerExample:25 - 	     partition: 1
2018-08-30 18:32:03 INFO  KafkaConsumerExample:26 - 	     offset: 137767
2018-08-30 18:32:03 INFO  KafkaConsumerExample:27 - 	     value: Hello world - 108
2018-08-30 18:32:04 INFO  KafkaConsumerExample:24 - Received message:
2018-08-30 18:32:04 INFO  KafkaConsumerExample:25 - 	     partition: 0
2018-08-30 18:32:04 INFO  KafkaConsumerExample:26 - 	     offset: 137774
2018-08-30 18:32:04 INFO  KafkaConsumerExample:27 - 	     value: Hello world - 109
2018-08-30 18:32:06 INFO  KafkaConsumerExample:24 - Received message:
2018-08-30 18:32:06 INFO  KafkaConsumerExample:25 - 	     partition: 1
2018-08-30 18:32:06 INFO  KafkaConsumerExample:26 - 	     offset: 137768
2018-08-30 18:32:06 INFO  KafkaConsumerExample:27 - 	     value: Hello world - 111
2018-08-30 18:32:07 INFO  KafkaConsumerExample:24 - Received message:
2018-08-30 18:32:07 INFO  KafkaConsumerExample:25 - 	     partition: 0
2018-08-30 18:32:07 INFO  KafkaConsumerExample:26 - 	     offset: 137775
2018-08-30 18:32:07 INFO  KafkaConsumerExample:27 - 	     value: Hello world - 112
----

and

[source,text]
----
2018-08-30 18:24:42 INFO  KafkaConsumerExample:24 - Received message:
2018-08-30 18:24:42 INFO  KafkaConsumerExample:25 - 	     partition: 2
2018-08-30 18:24:42 INFO  KafkaConsumerExample:26 - 	     offset: 137630
2018-08-30 18:24:42 INFO  KafkaConsumerExample:27 - 	     value: Hello world - 674
2018-08-30 18:24:45 INFO  KafkaConsumerExample:24 - Received message:
2018-08-30 18:24:45 INFO  KafkaConsumerExample:25 - 	     partition: 2
2018-08-30 18:24:45 INFO  KafkaConsumerExample:26 - 	     offset: 137631
2018-08-30 18:24:45 INFO  KafkaConsumerExample:27 - 	     value: Hello world - 677
2018-08-30 18:24:48 INFO  KafkaConsumerExample:24 - Received message:
2018-08-30 18:24:48 INFO  KafkaConsumerExample:25 - 	     partition: 2
2018-08-30 18:24:48 INFO  KafkaConsumerExample:26 - 	     offset: 137632
2018-08-30 18:24:48 INFO  KafkaConsumerExample:27 - 	     value: Hello world - 677
----

=== Configure Kafka connector

. In the resources/kafka-connect/kafka-connect.yaml, under the spec object, review the configuration
+
[source,text]
----
  config:
    key.converter: org.apache.kafka.connect.storage.StringConverter
    value.converter: org.apache.kafka.connect.storage.StringConverter
    key.converter.schemas.enable: false
    value.converter.schemas.enable: false
----

. Open a terminal in the Kakfa connect pod and verify that the input file has been copied to the /opt/kafka directory.

. A topic (file-publish) that Kafka Connect uses to publish to the Kafka broker has been created for you

. Create the configuration for the file source
+
[source,json]
----
% cat <<EOF >> /tmp/source-plugin.json
{
  "name": "source-test",
  "config": {
    "connector.class": "FileStreamSource",
    "tasks.max": "3",
    "topic": "UberInput",
    "file": "/tmp/uber.csv"
  }
}
EOF
----

. Create a connector that will read the TXT file and push its content into the Kafka broker
+
[source,text]
----
% curl -X POST -H "Content-Type: application/json" --data @/tmp/source-plugin.json http://localhost:8083/connectors
----

. Verify the contents are being published to the message consumer configured.
+
2018-08-21 22:08:26 INFO  KafkaConsumerExample:27 -	value: {"schema":{"type":"string","optional":false},"payload":"{1, 100, \"nandan\", \"uber data\", 15}"}

== Conclusions

== Questions

TO-DO :  questions to test student knowledge of the concepts / learning objectives of this lab

== Appendix
ifdef::showscript[]

endif::showscript[]
